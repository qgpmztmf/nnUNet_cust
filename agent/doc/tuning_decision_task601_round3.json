{
  "diagnosis": {
    "global_issue": "Catastrophic training failure - model has completely collapsed for nearly all 104 classes. Only 5 classes show non-zero Dice (0.2339 max), with 99 classes at 0.0000. This indicates either: 1) Insufficient training time (max_epochs=50 is far below nnUNet's standard 1000), 2) Learning rate too high causing divergence, 3) Loss function misconfiguration, or 4) Severe class imbalance with inadequate foreground sampling.",
    "class_specific_issues": {
      "class_5": "Best performing class at 0.2339 - indicates model can learn something but fails for most structures",
      "class_13": "Second best at 0.1555",
      "class_17": "Third best at 0.2132",
      "class_14": "0.0223 - barely detectable",
      "class_15": "0.0109 - barely detectable",
      "class_57": "0.0004 - essentially zero",
      "all_other_classes": "0.0000 - complete collapse"
    }
  },
  "tuning_decision": {
    "max_num_epochs": {
      "current_value": "50",
      "new_value": "200",
      "change_type": "increase",
      "reason": "Current 50 epochs is insufficient for 104-class segmentation. nnUNet standard is 1000 epochs. Increasing to 200 provides minimum viable training time while allowing faster iteration.",
      "expected_effect": "Allow model to converge properly; enable learning of more classes; improve overall mean Dice from near-zero"
    },
    "initial_lr_nnUNetTrainerV2": {
      "current_value": "0.01",
      "new_value": "0.001",
      "change_type": "decrease",
      "reason": "With 104 classes and catastrophic failure, 0.01 learning rate may be too aggressive, causing divergence. Reducing by factor of 10 provides more stable convergence.",
      "expected_effect": "Prevent gradient explosion; enable stable weight updates; allow model to learn gradually"
    },
    "oversample_foreground_percent": {
      "current_value": "0.33",
      "new_value": "0.5",
      "change_type": "increase",
      "reason": "With 104 classes, many small structures likely get ignored. Increasing foreground sampling ensures each batch contains more positive examples.",
      "expected_effect": "Improve learning of minority classes; reduce class imbalance effects; increase recall for small structures"
    },
    "batch_dice": {
      "current_value": "true",
      "new_value": "false",
      "change_type": "disable",
      "reason": "Batch Dice can be unstable with extreme class imbalance (104 classes). Sample-wise Dice provides more stable gradient for rare classes.",
      "expected_effect": "More stable gradient computation; better handling of class imbalance; prevent loss collapse"
    },
    "sgd_momentum": {
      "current_value": "0.99",
      "new_value": "0.9",
      "change_type": "decrease",
      "reason": "High momentum (0.99) with failed training can perpetuate bad updates. Reducing momentum allows faster correction of poor gradients.",
      "expected_effect": "More responsive to current gradient direction; faster recovery from poor initialization; improved convergence stability"
    }
  },
  "training_strategy": {
    "priority_level": "high",
    "risk_level": "medium",
    "retrain_required": true
  }
}
