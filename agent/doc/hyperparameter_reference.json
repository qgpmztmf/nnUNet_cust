{
  "max_num_epochs": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:97 | overridden in nnUNetTrainerV2.__init__:48 | nnUNetTrainerV2_fast.__init__:10 | nnUNetTrainerV2_configurable.__init__:97",
    "default_value": "1000 (NetworkTrainer) | 1000 (nnUNetTrainerV2) | 1000 (nnUNetTrainerV2_fast) | 1000 (nnUNetTrainerV2_configurable)",
    "value_range": "[100, 10000]",
    "category": "Training Loop Control",
    "description": "Maximum number of training epochs. nnUNetTrainerV2 always trains to this limit (no early stopping by default).",
    "active_value": "200"
  },
  "num_batches_per_epoch": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:98 | overridden in nnUNetTrainerV2_fast.__init__:11 | nnUNetTrainerV2_configurable.__init__:98",
    "default_value": "250 (NetworkTrainer) | 32 (nnUNetTrainerV2_fast / nnUNetTrainerV2_configurable)",
    "value_range": "[8, 1000]",
    "category": "Training Loop Control",
    "description": "Number of training iterations (minibatches) per epoch.",
    "active_value": 32
  },
  "num_val_batches_per_epoch": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:99 | overridden in nnUNetTrainerV2_fast.__init__:12 | nnUNetTrainerV2_configurable.__init__:99",
    "default_value": "50 (NetworkTrainer) | 1 (nnUNetTrainerV2_fast / nnUNetTrainerV2_configurable)",
    "value_range": "[1, 200]",
    "category": "Training Loop Control",
    "description": "Number of online validation iterations per epoch for loss tracking.",
    "active_value": 1
  },
  "also_val_in_tr_mode": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:100 | nnUNetTrainerV2_configurable.__init__:100",
    "default_value": "false",
    "value_range": [
      true,
      false
    ],
    "category": "Training Loop Control",
    "description": "Also compute validation loss with the network in training mode (adds train=True val losses to tracking)."
  },
  "patience": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:91 | nnUNetTrainerV2_configurable.__init__:101",
    "default_value": "50",
    "value_range": "[10, 500] or null to disable",
    "category": "Early Stopping",
    "description": "Epochs to wait without training loss MA improvement before early stopping. Disabled in nnUNetTrainerV2 by epoch override."
  },
  "val_eval_criterion_alpha": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:92 | nnUNetTrainerV2_configurable.__init__:102",
    "default_value": "0.9",
    "value_range": "(0.5, 0.99)",
    "category": "Early Stopping",
    "description": "EMA coefficient for validation evaluation criterion moving average. Higher = smoother but slower to react."
  },
  "train_loss_MA_alpha": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:95 | nnUNetTrainerV2_configurable.__init__:103",
    "default_value": "0.93",
    "value_range": "(0.7, 0.99)",
    "category": "Early Stopping",
    "description": "EMA coefficient for training loss moving average used for patience-based stopping."
  },
  "train_loss_MA_eps": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:96 | nnUNetTrainerV2_configurable.__init__:104",
    "default_value": "5e-4",
    "value_range": "[1e-5, 1e-2]",
    "category": "Early Stopping",
    "description": "Minimum improvement in train loss MA required to reset patience counter."
  },
  "lr_threshold": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:101 | nnUNetTrainerV2_configurable.__init__:105",
    "default_value": "1e-6",
    "value_range": "[1e-8, 1e-4]",
    "category": "Early Stopping",
    "description": "LR floor: early stopping only triggers if current LR is at or below this value."
  },
  "optimizer_type_nnUNetTrainer": {
    "location": "nnUNetTrainer.py \u2192 initialize_optimizer_and_scheduler:267",
    "default_value": "Adam (amsgrad=True)",
    "value_range": [
      "Adam",
      "SGD",
      "AdamW",
      "Ranger"
    ],
    "category": "Optimizer",
    "description": "Optimizer used in base nnUNetTrainer. Replaced by SGD in nnUNetTrainerV2."
  },
  "optimizer_type_nnUNetTrainerV2": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_optimizer_and_scheduler:166 | nnUNetTrainerV2_configurable.py \u2192 initialize_optimizer_and_scheduler:638",
    "default_value": "SGD",
    "value_range": [
      "SGD",
      "Adam",
      "AdamW"
    ],
    "category": "Optimizer",
    "description": "Optimizer used in nnUNetTrainerV2 and subclasses. SGD with high momentum and poly-LR schedule."
  },
  "initial_lr_nnUNetTrainer": {
    "location": "nnUNetTrainer.py \u2192 __init__:126",
    "default_value": "3e-4",
    "value_range": "[1e-5, 1e-2]",
    "category": "Optimizer",
    "description": "Initial learning rate for Adam optimizer in base nnUNetTrainer."
  },
  "initial_lr_nnUNetTrainerV2": {
    "location": "nnUNetTrainerV2.py \u2192 __init__:49 | nnUNetTrainerV2_configurable.__init__:108",
    "default_value": "1e-2",
    "value_range": "[1e-3, 3e-2]",
    "category": "Optimizer",
    "description": "Initial learning rate for SGD optimizer in nnUNetTrainerV2 with poly-LR decay.",
    "active_value": "0.01"
  },
  "weight_decay": {
    "location": "nnUNetTrainer.py \u2192 __init__:127 | nnUNetTrainerV2_configurable.__init__:109",
    "default_value": "3e-5",
    "value_range": "[1e-6, 1e-3]",
    "category": "Optimizer",
    "description": "L2 weight decay regularization, shared by both nnUNetTrainer and nnUNetTrainerV2."
  },
  "sgd_momentum": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_optimizer_and_scheduler:167 | nnUNetTrainerV2_configurable.__init__:110 | nnUNetTrainerV2_configurable.py \u2192 initialize_optimizer_and_scheduler:643",
    "default_value": "0.99",
    "value_range": "[0.9, 0.999]",
    "category": "Optimizer",
    "description": "SGD momentum. Auto-reduced to 0.95 at epoch 100 if val Dice is 0.",
    "active_value": "0.99"
  },
  "sgd_nesterov": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_optimizer_and_scheduler:167 | nnUNetTrainerV2_configurable.__init__:111 | nnUNetTrainerV2_configurable.py \u2192 initialize_optimizer_and_scheduler:644",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Optimizer",
    "description": "Enable Nesterov momentum in SGD."
  },
  "adam_amsgrad": {
    "location": "nnUNetTrainer.py \u2192 initialize_optimizer_and_scheduler:268",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Optimizer",
    "description": "Use AMSGrad variant of Adam (base nnUNetTrainer only)."
  },
  "poly_lr_exponent": {
    "location": "nnUNetTrainerV2.py \u2192 maybe_update_lr:405 | poly_lr.py \u2192 poly_lr:17 | nnUNetTrainerV2_configurable.__init__:112",
    "default_value": "0.9",
    "value_range": "(0.5, 1.0)",
    "category": "Learning Rate Schedule",
    "description": "Exponent for polynomial LR decay: lr = initial_lr * (1 - epoch/max_epochs)^exponent."
  },
  "lr_scheduler_type_nnUNetTrainer": {
    "location": "nnUNetTrainer.py \u2192 initialize_optimizer_and_scheduler:269",
    "default_value": "ReduceLROnPlateau",
    "value_range": [
      "ReduceLROnPlateau",
      "CosineAnnealingLR",
      "StepLR",
      "None"
    ],
    "category": "Learning Rate Schedule",
    "description": "LR scheduler used in base nnUNetTrainer. Not used in nnUNetTrainerV2 (uses poly_lr instead)."
  },
  "lr_scheduler_factor": {
    "location": "nnUNetTrainer.py \u2192 initialize_optimizer_and_scheduler:270",
    "default_value": "0.2",
    "value_range": "(0.05, 0.5)",
    "category": "Learning Rate Schedule",
    "description": "Multiplicative factor for ReduceLROnPlateau LR reduction."
  },
  "lr_scheduler_patience": {
    "location": "nnUNetTrainer.py \u2192 __init__:125 | nnUNetTrainerV2_configurable.__init__:197",
    "default_value": "30",
    "value_range": "[5, 100]",
    "category": "Learning Rate Schedule",
    "description": "Patience for ReduceLROnPlateau before reducing LR (nnUNetTrainer only; nnUNetTrainerV2 uses poly_lr)."
  },
  "lr_scheduler_eps": {
    "location": "nnUNetTrainer.py \u2192 __init__:124 | nnUNetTrainerV2_configurable.__init__:197",
    "default_value": "1e-3",
    "value_range": "[1e-5, 1e-2]",
    "category": "Learning Rate Schedule",
    "description": "Threshold for ReduceLROnPlateau (threshold_mode=abs). Stored on nnUNetTrainerV2_configurable for completeness."
  },
  "momentum_rescue_threshold": {
    "location": "nnUNetTrainerV2.py \u2192 on_epoch_end:420 | nnUNetTrainerV2_configurable.__init__:114 | nnUNetTrainerV2_configurable.py \u2192 on_epoch_end:720",
    "default_value": "0.95 (applied when val Dice = 0 at epoch 100)",
    "value_range": "(0.9, 0.99)",
    "category": "Learning Rate Schedule",
    "description": "If foreground Dice is still 0 at epoch 100, momentum is reduced to this value and network weights are reinitialised."
  },
  "grad_clip_norm": {
    "location": "nnUNetTrainerV2.py \u2192 run_iteration:254,264 | nnUNetTrainerV2_configurable.__init__:113 | nnUNetTrainerV2_configurable.py \u2192 run_iteration:791,800",
    "default_value": "12",
    "value_range": "[1, 50]",
    "category": "Gradient Clipping",
    "description": "Maximum gradient norm for torch.nn.utils.clip_grad_norm_. Applied every training iteration."
  },
  "fp16": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:59",
    "default_value": "false",
    "value_range": [
      true,
      false
    ],
    "category": "Mixed Precision / Reproducibility",
    "description": "Enable AMP training (torch.cuda.amp.autocast + GradScaler). Also controls mixed-precision inference."
  },
  "deterministic": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:62-68",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Mixed Precision / Reproducibility",
    "description": "If true: seeds NumPy/PyTorch/CUDA to 12345, enables cudnn.deterministic, disables cudnn.benchmark."
  },
  "random_seed": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:63-66",
    "default_value": "12345",
    "value_range": "any integer",
    "category": "Mixed Precision / Reproducibility",
    "description": "Hardcoded seed for NumPy, torch.manual_seed, and torch.cuda.manual_seed_all when deterministic=True."
  },
  "pin_memory": {
    "location": "nnUNetTrainerV2.py \u2192 __init__:53 | nnUNetTrainerV2_configurable.__init__:187",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Mixed Precision / Reproducibility",
    "description": "Pin DataLoader memory for faster host-to-GPU transfer. Passed to get_moreDA_augmentation."
  },
  "save_every": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:122 | nnUNetTrainerV2_configurable.__init__:190",
    "default_value": "50",
    "value_range": "[10, 200]",
    "category": "Checkpointing",
    "description": "Save intermediate checkpoint every N epochs."
  },
  "save_latest_only": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:123 | nnUNetTrainerV2_configurable.__init__:191",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Checkpointing",
    "description": "Only keep model_latest.model; do not save per-epoch checkpoint files."
  },
  "save_intermediate_checkpoints": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:125 | nnUNetTrainerV2_configurable.__init__:192",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Checkpointing",
    "description": "Periodically save model_latest.model every save_every epochs."
  },
  "save_best_checkpoint": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:126 | nnUNetTrainerV2_configurable.__init__:193",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Checkpointing",
    "description": "Save model_best.model when validation criterion MA improves."
  },
  "save_final_checkpoint": {
    "location": "network_trainer.py \u2192 NetworkTrainer.__init__:127 | nnUNetTrainerV2_configurable.__init__:194",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Checkpointing",
    "description": "Save model_final_checkpoint.model at end of training."
  },
  "loss_function": {
    "location": "nnUNetTrainer.py \u2192 __init__:108 | nnUNetTrainerV2_configurable.__init__:125",
    "default_value": "DC_and_CE_loss",
    "value_range": [
      "DC_and_CE_loss",
      "DC_and_topk_loss",
      "GDL_and_CE_loss",
      "DC_and_BCE_loss",
      "SoftDiceLoss",
      "RobustCrossEntropyLoss"
    ],
    "category": "Loss Function",
    "description": "Combined SoftDice + CrossEntropy loss. Wrapped with MultipleOutputLoss2 in nnUNetTrainerV2 for deep supervision."
  },
  "batch_dice": {
    "location": "nnUNetTrainer.py \u2192 __init__:107 | nnUNetTrainerV2_configurable.__init__:86",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Loss Function",
    "description": "Compute Dice loss pooling across the entire batch (pseudo-volume) rather than per sample.",
    "active_value": "true"
  },
  "dice_smooth": {
    "location": "nnUNetTrainer.py \u2192 __init__:108 | nnUNetTrainerV2_configurable.__init__:118",
    "default_value": "1e-5",
    "value_range": "[0, 1e-1]",
    "category": "Loss Function",
    "description": "Laplace smoothing constant in SoftDice numerator and denominator."
  },
  "dice_do_bg": {
    "location": "nnUNetTrainer.py \u2192 __init__:108 | nnUNetTrainerV2_configurable.__init__:119",
    "default_value": "false",
    "value_range": [
      true,
      false
    ],
    "category": "Loss Function",
    "description": "Include background class (index 0) in Dice computation. False improves focus on foreground."
  },
  "weight_dice": {
    "location": "dice_loss.py \u2192 DC_and_CE_loss.__init__:305 | nnUNetTrainerV2_configurable.__init__:120",
    "default_value": "1",
    "value_range": "[0, 10]",
    "category": "Loss Function",
    "description": "Scalar weight applied to the Dice component of the combined loss."
  },
  "weight_ce": {
    "location": "dice_loss.py \u2192 DC_and_CE_loss.__init__:305 | nnUNetTrainerV2_configurable.__init__:121",
    "default_value": "1",
    "value_range": "[0, 10]",
    "category": "Loss Function",
    "description": "Scalar weight applied to the CrossEntropy component of the combined loss.",
    "active_value": "2.0"
  },
  "square_dice": {
    "location": "dice_loss.py \u2192 DC_and_CE_loss.__init__:305 | nnUNetTrainerV2_configurable.__init__:122",
    "default_value": "false",
    "value_range": [
      true,
      false
    ],
    "category": "Loss Function",
    "description": "Use SoftDiceLossSquared (Milletari et al.) instead of standard SoftDiceLoss."
  },
  "log_dice": {
    "location": "dice_loss.py \u2192 DC_and_CE_loss.__init__:305 | nnUNetTrainerV2_configurable.__init__:123",
    "default_value": "false",
    "value_range": [
      true,
      false
    ],
    "category": "Loss Function",
    "description": "Apply -log transform to the Dice loss to emphasize hard examples."
  },
  "topk_k": {
    "location": "TopK_loss.py \u2192 TopKLoss.__init__:24 | nnUNetTrainerV2_configurable.__init__:124",
    "default_value": "10",
    "value_range": "[1, 50] (percent)",
    "category": "Loss Function",
    "description": "Top-K percentage of hardest voxels to include in TopK CrossEntropy loss (used when loss_function=DC_and_topk_loss)."
  },
  "deep_supervision_enabled": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_network:158 | nnUNetTrainerV2_configurable.__init__:128 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:618",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Deep Supervision",
    "description": "Enable multi-scale deep supervision outputs from the Generic_UNet decoder."
  },
  "ds_loss_weight_base": {
    "location": "nnUNetTrainerV2.py \u2192 initialize:81 | nnUNetTrainerV2_configurable.py \u2192 initialize:520",
    "default_value": "1/2^i (exponential decay per level)",
    "value_range": "custom array of positive floats summing to 1",
    "category": "Deep Supervision",
    "description": "Per-scale loss weights: scale i gets weight 1/2^i; lowest level(s) masked to 0 then renormalized."
  },
  "ds_lowest_levels_masked": {
    "location": "nnUNetTrainerV2.py \u2192 initialize:84 | nnUNetTrainerV2_configurable.__init__:129 | nnUNetTrainerV2_configurable.py \u2192 initialize:524",
    "default_value": "1 (lowest resolution level masked)",
    "value_range": "[0, num_pool-1]",
    "category": "Deep Supervision",
    "description": "Number of lowest-resolution deep supervision outputs excluded from loss computation."
  },
  "base_num_features_v21": {
    "location": "experiment_planner_baseline_3DUNet_v21.py:36",
    "default_value": "32",
    "value_range": "[8, 64] (multiples of 8 recommended for AMP)",
    "category": "Network Architecture",
    "description": "Initial number of feature maps. 32 (vs 30) chosen for AMP divisibility-by-8 speedup."
  },
  "base_num_features_base": {
    "location": "generic_UNet.py \u2192 Generic_UNet:171 | experiment_planner_baseline_3DUNet.py:52",
    "default_value": "30",
    "value_range": "[8, 64]",
    "category": "Network Architecture",
    "description": "Base feature map count class constant for the default planner (non-v21)."
  },
  "max_num_features_3D": {
    "location": "generic_UNet.py \u2192 Generic_UNet:173",
    "default_value": "320",
    "value_range": "[64, 512]",
    "category": "Network Architecture",
    "description": "Cap on maximum number of feature maps for 3D networks (MAX_NUM_FILTERS_3D class constant)."
  },
  "max_num_features_2D": {
    "location": "generic_UNet.py \u2192 Generic_UNet:179",
    "default_value": "480",
    "value_range": "[64, 512]",
    "category": "Network Architecture",
    "description": "Cap on maximum number of feature maps for 2D networks (MAX_FILTERS_2D class constant)."
  },
  "feat_map_mul_on_downscale": {
    "location": "generic_UNet.py \u2192 Generic_UNet.__init__:184 | nnUNetTrainerV2.py \u2192 initialize_network:156 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:610",
    "default_value": "2",
    "value_range": "[1, 4]",
    "category": "Network Architecture",
    "description": "Feature map doubling factor applied at each encoder pooling stage."
  },
  "num_conv_per_stage": {
    "location": "generic_UNet.py \u2192 Generic_UNet.__init__:184 | nnUNetTrainer.py \u2192 process_plans:390-392",
    "default_value": "2",
    "value_range": "[1, 4]",
    "category": "Network Architecture",
    "description": "Number of convolutional blocks per encoder and decoder stage."
  },
  "norm_op": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_network:143,148 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:579,583",
    "default_value": "InstanceNorm3d (3D) / InstanceNorm2d (2D)",
    "value_range": [
      "InstanceNorm3d",
      "BatchNorm3d",
      "GroupNorm"
    ],
    "category": "Network Architecture",
    "description": "Normalization layer applied after each convolution."
  },
  "norm_op_eps": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_network:150 | nnUNetTrainerV2_configurable.__init__:133 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:585",
    "default_value": "1e-5",
    "value_range": "[1e-7, 1e-3]",
    "category": "Network Architecture",
    "description": "Epsilon for numerical stability in InstanceNorm."
  },
  "norm_op_affine": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_network:150 | nnUNetTrainerV2_configurable.__init__:134 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:585",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Network Architecture",
    "description": "Learn affine scale (gamma) and shift (beta) parameters in InstanceNorm."
  },
  "dropout_p": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_network:151 | nnUNetTrainerV2_configurable.__init__:135 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:586",
    "default_value": "0",
    "value_range": "[0.0, 0.5]",
    "category": "Network Architecture",
    "description": "Dropout probability. Set to 0 by default (disabled). Decoder dropout separately controlled by dropout_in_localization."
  },
  "dropout_in_localization": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_network:158 | nnUNetTrainerV2_configurable.__init__:136 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:619",
    "default_value": "false (nnUNetTrainerV2) | true (nnUNetTrainerV2_configurable default)",
    "value_range": [
      true,
      false
    ],
    "category": "Network Architecture",
    "description": "Apply dropout in the decoder/localization path. When false, dropout_p is forced to 0.0 in decoder."
  },
  "nonlin": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_network:152 | nnUNetTrainerV2_configurable.__init__:137 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:589-594",
    "default_value": "nn.LeakyReLU",
    "value_range": [
      "nn.LeakyReLU",
      "nn.ReLU",
      "nn.ELU",
      "nn.GELU",
      "nn.Mish"
    ],
    "category": "Network Architecture",
    "description": "Activation function applied after each normalization layer."
  },
  "nonlin_negative_slope": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_network:153 | nnUNetTrainerV2_configurable.__init__:138 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:598",
    "default_value": "1e-2",
    "value_range": "[0.0, 0.5]",
    "category": "Network Architecture",
    "description": "Negative slope coefficient for LeakyReLU."
  },
  "convolutional_pooling": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_network:158 | nnUNetTrainerV2_configurable.__init__:139 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:625",
    "default_value": "false (nnUNetTrainerV2) | true (nnUNetTrainerV2_configurable default)",
    "value_range": [
      true,
      false
    ],
    "category": "Network Architecture",
    "description": "Use strided convolution instead of MaxPooling for downsampling."
  },
  "convolutional_upsampling": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_network:159 | nnUNetTrainerV2_configurable.__init__:140 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:626",
    "default_value": "false (nnUNetTrainerV2) | true (nnUNetTrainerV2_configurable default)",
    "value_range": [
      true,
      false
    ],
    "category": "Network Architecture",
    "description": "Use transposed convolution instead of bilinear/trilinear interpolation for upsampling."
  },
  "seg_output_use_bias": {
    "location": "generic_UNet.py \u2192 Generic_UNet.__init__:193 | nnUNetTrainerV2_configurable.__init__:141",
    "default_value": "false",
    "value_range": [
      true,
      false
    ],
    "category": "Network Architecture",
    "description": "Add bias to the final segmentation output convolution layer."
  },
  "he_init_neg_slope": {
    "location": "nnUNetTrainerV2.py \u2192 initialize_network:158 | nnUNetTrainerV2_configurable.__init__:142 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:621",
    "default_value": "1e-2",
    "value_range": "[0.0, 0.2]",
    "category": "Network Architecture",
    "description": "Negative slope parameter for Kaiming (He) normal weight initialization, matched to LeakyReLU slope."
  },
  "upscale_logits": {
    "location": "generic_UNet.py \u2192 Generic_UNet.__init__:191 | nnUNetTrainerV2.py \u2192 initialize_network:159 | nnUNetTrainerV2_configurable.py \u2192 initialize_network:624",
    "default_value": "false",
    "value_range": [
      true,
      false
    ],
    "category": "Network Architecture",
    "description": "Upscale all deep supervision outputs to full resolution before loss computation. Default false."
  },
  "do_elastic": {
    "location": "default_data_augmentation.py:43 | overridden in nnUNetTrainerV2.py \u2192 setup_DA_params:385 | nnUNetTrainerV2_configurable.__init__:146",
    "default_value": "true (default) | false (nnUNetTrainerV2 and subclasses)",
    "value_range": [
      true,
      false
    ],
    "category": "Data Augmentation",
    "description": "Enable elastic deformation augmentation. Disabled in nnUNetTrainerV2 and subclasses."
  },
  "elastic_deform_alpha": {
    "location": "default_data_augmentation.py:43 | nnUNetTrainerV2_configurable.__init__:147",
    "default_value": "(0.0, 900.0)",
    "value_range": "[(0, 100), (0, 2000)]",
    "category": "Data Augmentation",
    "description": "Alpha range for elastic deformation magnitude."
  },
  "elastic_deform_sigma": {
    "location": "default_data_augmentation.py:44 | nnUNetTrainerV2_configurable.__init__:148",
    "default_value": "(9.0, 13.0)",
    "value_range": "[(5, 10), (10, 20)]",
    "category": "Data Augmentation",
    "description": "Sigma range for elastic deformation smoothness."
  },
  "p_eldef": {
    "location": "default_data_augmentation.py:45 | nnUNetTrainerV2_configurable.__init__:149",
    "default_value": "0.2",
    "value_range": "[0.0, 0.5]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of applying elastic deformation."
  },
  "do_scaling": {
    "location": "default_data_augmentation.py:47 | nnUNetTrainerV2_configurable.__init__:150",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Data Augmentation",
    "description": "Enable random scaling augmentation."
  },
  "scale_range": {
    "location": "default_data_augmentation.py:48 | overridden in nnUNetTrainerV2.py \u2192 setup_DA_params:384 | nnUNetTrainerV2_configurable.__init__:151",
    "default_value": "(0.85, 1.25) default | (0.7, 1.4) nnUNetTrainerV2 and subclasses",
    "value_range": "[(0.5, 1.0), (1.0, 2.0)] pair",
    "category": "Data Augmentation",
    "description": "Min/max scaling factor range. nnUNetTrainerV2 uses wider range (0.7, 1.4)."
  },
  "p_scale": {
    "location": "default_data_augmentation.py:52 | nnUNetTrainerV2_configurable.__init__:152",
    "default_value": "0.2",
    "value_range": "[0.0, 0.5]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of applying random scaling."
  },
  "independent_scale_factor_for_each_axis": {
    "location": "default_data_augmentation.py:49 | nnUNetTrainerV2_configurable.__init__:153",
    "default_value": "false",
    "value_range": [
      true,
      false
    ],
    "category": "Data Augmentation",
    "description": "Scale each spatial axis independently rather than uniformly."
  },
  "do_rotation": {
    "location": "default_data_augmentation.py:54 | nnUNetTrainerV2_configurable.__init__:154",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Data Augmentation",
    "description": "Enable random rotation augmentation."
  },
  "rotation_x_default": {
    "location": "default_data_augmentation.py:55",
    "default_value": "(-0.2618, 0.2618) radians = \u00b115 degrees",
    "value_range": "[(-pi/2, pi/2)]",
    "category": "Data Augmentation",
    "description": "Rotation range around x-axis in radians (default 3D augmentation)."
  },
  "rotation_y_default": {
    "location": "default_data_augmentation.py:56",
    "default_value": "(-0.2618, 0.2618) radians = \u00b115 degrees",
    "value_range": "[(-pi/2, pi/2)]",
    "category": "Data Augmentation",
    "description": "Rotation range around y-axis in radians."
  },
  "rotation_z_default": {
    "location": "default_data_augmentation.py:57",
    "default_value": "(-0.2618, 0.2618) radians = \u00b115 degrees",
    "value_range": "[(-pi/2, pi/2)]",
    "category": "Data Augmentation",
    "description": "Rotation range around z-axis in radians."
  },
  "rotation_x_V2": {
    "location": "nnUNetTrainerV2.py \u2192 setup_DA_params:353 | nnUNetTrainerV2_configurable.__init__:155",
    "default_value": "(-0.5236, 0.5236) radians = \u00b130 degrees",
    "value_range": "[(-pi/2, pi/2)]",
    "category": "Data Augmentation",
    "description": "Increased x-axis rotation range in nnUNetTrainerV2 and subclasses."
  },
  "rotation_p_per_axis": {
    "location": "default_data_augmentation.py:58 | nnUNetTrainerV2_configurable.__init__:158",
    "default_value": "1",
    "value_range": "[0.0, 1.0]",
    "category": "Data Augmentation",
    "description": "Probability of applying rotation to each individual axis."
  },
  "p_rot": {
    "location": "default_data_augmentation.py:59 | nnUNetTrainerV2_configurable.__init__:159",
    "default_value": "0.2",
    "value_range": "[0.0, 0.5]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of applying rotation."
  },
  "do_gamma": {
    "location": "default_data_augmentation.py:64 | nnUNetTrainerV2_configurable.__init__:161",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Data Augmentation",
    "description": "Enable gamma augmentation (power-law intensity transform)."
  },
  "gamma_range": {
    "location": "default_data_augmentation.py:65 | nnUNetTrainerV2_configurable.__init__:162",
    "default_value": "(0.7, 1.5)",
    "value_range": "[(0.5, 1.0), (1.0, 2.5)] pair",
    "category": "Data Augmentation",
    "description": "Range for gamma values in gamma augmentation."
  },
  "gamma_retain_stats": {
    "location": "default_data_augmentation.py:65 | nnUNetTrainerV2_configurable.__init__:163",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Data Augmentation",
    "description": "Rescale gamma-transformed image to match original mean and std."
  },
  "p_gamma": {
    "location": "default_data_augmentation.py:67 | nnUNetTrainerV2_configurable.__init__:164",
    "default_value": "0.3",
    "value_range": "[0.0, 0.5]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of forward gamma augmentation."
  },
  "gamma_inverted_p": {
    "location": "data_augmentation_moreDA.py:83 (hardcoded) | nnUNetTrainerV2_configurable.__init__:165",
    "default_value": "0.1",
    "value_range": "[0.0, 0.3]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of inverted gamma augmentation (applied before forward gamma in moreDA). Hardcoded in moreDA."
  },
  "do_mirror": {
    "location": "default_data_augmentation.py:69 | overridden in nnUNetTrainerV2_fast.py \u2192 setup_DA_params:16 | nnUNetTrainerV2_configurable.__init__:170",
    "default_value": "true (default) | false (nnUNetTrainerV2_fast and configurable)",
    "value_range": [
      true,
      false
    ],
    "category": "Data Augmentation",
    "description": "Enable random mirroring augmentation during training and TTA during inference."
  },
  "mirror_axes": {
    "location": "default_data_augmentation.py:70 | nnUNetTrainerV2_configurable.__init__:171",
    "default_value": "(0, 1, 2) for 3D | (0, 1) for 2D",
    "value_range": "any subset of (0,) (1,) (2,) (0,1) (0,2) (1,2) (0,1,2)",
    "category": "Data Augmentation",
    "description": "Axes along which random mirroring may be applied."
  },
  "dummy_2D": {
    "location": "default_data_augmentation.py:72 | nnUNetTrainerV2_configurable.__init__:172",
    "default_value": "false",
    "value_range": [
      true,
      false
    ],
    "category": "Data Augmentation",
    "description": "Treat 3D patches as 2D slices for augmentation; used for highly anisotropic data."
  },
  "border_mode_data": {
    "location": "default_data_augmentation.py:74 | nnUNetTrainerV2_configurable.__init__:173",
    "default_value": "constant",
    "value_range": [
      "constant",
      "nearest",
      "reflect",
      "wrap"
    ],
    "category": "Data Augmentation",
    "description": "Border handling mode for spatial transform (rotation/scaling/elastic)."
  },
  "do_additive_brightness": {
    "location": "default_data_augmentation.py:76 | nnUNetTrainerV2_configurable.__init__:166",
    "default_value": "false",
    "value_range": [
      true,
      false
    ],
    "category": "Data Augmentation",
    "description": "Enable additive brightness augmentation (forwarded via data_aug_params to moreDA)."
  },
  "additive_brightness_mu": {
    "location": "default_data_augmentation.py:79 | nnUNetTrainerV2_configurable.__init__:167",
    "default_value": "0.0",
    "value_range": "[-0.5, 0.5]",
    "category": "Data Augmentation",
    "description": "Mean of additive brightness distribution."
  },
  "additive_brightness_sigma": {
    "location": "default_data_augmentation.py:80 | nnUNetTrainerV2_configurable.__init__:168",
    "default_value": "0.1",
    "value_range": "[0.01, 0.5]",
    "category": "Data Augmentation",
    "description": "Std of additive brightness distribution."
  },
  "gaussian_noise_p": {
    "location": "data_augmentation_moreDA.py:79 (hardcoded) | nnUNetTrainerV2_configurable.__init__:176",
    "default_value": "0.1",
    "value_range": "[0.0, 0.3]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of Gaussian noise injection (hardcoded in moreDA; stored on configurable trainer for transparency)."
  },
  "gaussian_blur_sigma_range": {
    "location": "data_augmentation_moreDA.py:80 (hardcoded) | nnUNetTrainerV2_configurable.__init__:177",
    "default_value": "(0.5, 1.0)",
    "value_range": "[(0.1, 0.5), (0.5, 2.0)] pair",
    "category": "Data Augmentation",
    "description": "Sigma range for Gaussian blur (hardcoded in moreDA)."
  },
  "gaussian_blur_p_per_sample": {
    "location": "data_augmentation_moreDA.py:80 (hardcoded) | nnUNetTrainerV2_configurable.__init__:178",
    "default_value": "0.2",
    "value_range": "[0.0, 0.5]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of Gaussian blur (hardcoded in moreDA)."
  },
  "brightness_multiplicative_range": {
    "location": "data_augmentation_moreDA.py:81 (hardcoded) | nnUNetTrainerV2_configurable.__init__:179",
    "default_value": "(0.75, 1.25)",
    "value_range": "[(0.5, 1.0), (1.0, 2.0)] pair",
    "category": "Data Augmentation",
    "description": "Range for multiplicative brightness augmentation (hardcoded in moreDA)."
  },
  "brightness_multiplicative_p": {
    "location": "data_augmentation_moreDA.py:81 (hardcoded) | nnUNetTrainerV2_configurable.__init__:180",
    "default_value": "0.15",
    "value_range": "[0.0, 0.4]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of multiplicative brightness augmentation (hardcoded in moreDA)."
  },
  "contrast_augmentation_p": {
    "location": "data_augmentation_moreDA.py:84 (hardcoded) | nnUNetTrainerV2_configurable.__init__:181",
    "default_value": "0.15",
    "value_range": "[0.0, 0.4]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of contrast augmentation (hardcoded in moreDA)."
  },
  "simulate_lowres_zoom_range": {
    "location": "data_augmentation_moreDA.py:85 (hardcoded) | nnUNetTrainerV2_configurable.__init__:182",
    "default_value": "(0.5, 1.0)",
    "value_range": "[(0.25, 0.75), (0.5, 1.0)] pair",
    "category": "Data Augmentation",
    "description": "Zoom range for simulated low-resolution augmentation (hardcoded in moreDA)."
  },
  "simulate_lowres_p_per_sample": {
    "location": "data_augmentation_moreDA.py:85 (hardcoded) | nnUNetTrainerV2_configurable.__init__:183",
    "default_value": "0.25",
    "value_range": "[0.0, 0.5]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of low-resolution simulation (hardcoded in moreDA)."
  },
  "num_augmentation_threads": {
    "location": "default_data_augmentation.py:82",
    "default_value": "12 or $nnUNet_n_proc_DA",
    "value_range": "[1, 32]",
    "category": "Data Loading",
    "description": "Number of worker threads for MultiThreadedAugmenter. Controlled by env var nnUNet_n_proc_DA."
  },
  "num_cached_per_thread": {
    "location": "default_data_augmentation.py:83 | overridden in nnUNetTrainerV2.py \u2192 setup_DA_params:389 | nnUNetTrainerV2_configurable.__init__:174",
    "default_value": "1 (default) | 2 (nnUNetTrainerV2 and subclasses)",
    "value_range": "[1, 8]",
    "category": "Data Loading",
    "description": "Batches pre-fetched and cached per augmentation thread."
  },
  "oversample_foreground_percent": {
    "location": "nnUNetTrainer.py \u2192 __init__:129 | nnUNetTrainerV2_configurable.__init__:186",
    "default_value": "0.33",
    "value_range": "[0.0, 1.0]",
    "category": "Data Loading",
    "description": "Fraction of each batch guaranteed to contain foreground voxels for class balance.",
    "active_value": "0.75"
  },
  "unpack_data": {
    "location": "nnUNetTrainer.py \u2192 __init__:76",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Data Loading",
    "description": "Decompress NPZ files to NPY before training for faster I/O at the cost of disk space."
  },
  "use_nondetMultiThreadedAugmenter": {
    "location": "nnUNetTrainerV2.py \u2192 initialize:112 | nnUNetTrainerV2_configurable.py \u2192 initialize:555",
    "default_value": "false",
    "value_range": [
      true,
      false
    ],
    "category": "Data Loading",
    "description": "Use NonDetMultiThreadedAugmenter (faster but non-deterministic) instead of standard MultiThreadedAugmenter."
  },
  "n_cv_splits": {
    "location": "nnUNetTrainerV2.py \u2192 do_split:296",
    "default_value": "5",
    "value_range": "[3, 10]",
    "category": "Cross-Validation",
    "description": "Number of cross-validation folds."
  },
  "kfold_random_state": {
    "location": "nnUNetTrainerV2.py \u2192 do_split:296 (hardcoded)",
    "default_value": "12345",
    "value_range": "any integer",
    "category": "Cross-Validation",
    "description": "Fixed seed for KFold split to ensure reproducibility across runs."
  },
  "out_of_range_fold_train_ratio": {
    "location": "nnUNetTrainerV2.py \u2192 do_split:323 (hardcoded)",
    "default_value": "0.8",
    "value_range": "(0.5, 0.95)",
    "category": "Cross-Validation",
    "description": "Train fraction for fallback random split when requested fold index exceeds available splits."
  },
  "val_do_mirroring": {
    "location": "nnUNetTrainerV2.py \u2192 validate:182 | overridden in nnUNetTrainerV2_fast.py \u2192 validate:22",
    "default_value": "true (nnUNetTrainerV2) | false (nnUNetTrainerV2_fast and subclasses)",
    "value_range": [
      true,
      false
    ],
    "category": "Inference & Validation",
    "description": "Enable test-time augmentation (TTA) via mirroring during full validation. Forced to false in fast trainers."
  },
  "val_use_sliding_window": {
    "location": "nnUNetTrainer.py \u2192 validate:526",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Inference & Validation",
    "description": "Use sliding window inference for full-resolution validation (vs fully convolutional)."
  },
  "val_step_size": {
    "location": "nnUNetTrainer.py \u2192 validate:526",
    "default_value": "0.5",
    "value_range": "(0.1, 1.0)",
    "category": "Inference & Validation",
    "description": "Fraction of patch size used as step between sliding window positions (0.5 = 50% overlap)."
  },
  "val_use_gaussian": {
    "location": "nnUNetTrainer.py \u2192 validate:526",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Inference & Validation",
    "description": "Apply Gaussian importance weighting map in sliding window to reduce border artifacts."
  },
  "val_save_softmax": {
    "location": "nnUNetTrainer.py \u2192 validate:527",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Inference & Validation",
    "description": "Save softmax probability maps as NPZ files alongside hard segmentations."
  },
  "val_all_in_gpu": {
    "location": "nnUNetTrainer.py \u2192 validate:528",
    "default_value": "false",
    "value_range": [
      true,
      false
    ],
    "category": "Inference & Validation",
    "description": "Keep aggregated softmax on GPU throughout sliding window (requires sufficient VRAM)."
  },
  "inference_pad_border_mode": {
    "location": "nnUNetTrainer.py \u2192 __init__:118 | nnUNetTrainerV2_configurable.__init__:199",
    "default_value": "constant",
    "value_range": [
      "constant",
      "reflect",
      "edge"
    ],
    "category": "Inference & Validation",
    "description": "Padding mode applied at image borders during inference."
  },
  "inference_pad_constant_value": {
    "location": "nnUNetTrainer.py \u2192 __init__:119 | nnUNetTrainerV2_configurable.__init__:200",
    "default_value": "0",
    "value_range": "any float (typically 0 for CT or background value)",
    "category": "Inference & Validation",
    "description": "Fill value used when pad_border_mode is constant during inference."
  },
  "output_interpolation_order": {
    "location": "nnUNetTrainer.py \u2192 validate:549",
    "default_value": "1",
    "value_range": "[0, 3]",
    "category": "Inference & Validation",
    "description": "Spline interpolation order for resampling softmax predictions back to original spacing."
  },
  "output_interpolation_order_z": {
    "location": "nnUNetTrainer.py \u2192 validate:550",
    "default_value": "0",
    "value_range": "[0, 1]",
    "category": "Inference & Validation",
    "description": "Spline interpolation order along z-axis for anisotropic output resampling."
  },
  "run_postprocessing_on_folds": {
    "location": "nnUNetTrainer.py \u2192 validate:529",
    "default_value": "true",
    "value_range": [
      true,
      false
    ],
    "category": "Inference & Validation",
    "description": "Run connected component postprocessing (keep largest component per class) after validation."
  },
  "unet_base_num_features_planner": {
    "location": "experiment_planner_baseline_3DUNet.py:52 | experiment_planner_baseline_3DUNet_v21.py:36",
    "default_value": "30 (base) | 32 (v21)",
    "value_range": "[8, 64] (multiples of 8 for AMP)",
    "category": "Experiment Planning",
    "description": "Base feature count used during memory estimation in experiment planner."
  },
  "unet_max_num_filters_planner": {
    "location": "experiment_planner_baseline_3DUNet.py:53",
    "default_value": "320",
    "value_range": "[64, 512]",
    "category": "Experiment Planning",
    "description": "Maximum feature map count ceiling applied during architecture planning."
  },
  "unet_min_batch_size": {
    "location": "experiment_planner_baseline_3DUNet.py:55",
    "default_value": "2",
    "value_range": "[1, 8]",
    "category": "Experiment Planning",
    "description": "Minimum batch size planned by the experiment planner."
  },
  "unet_featuremap_min_edge_length": {
    "location": "experiment_planner_baseline_3DUNet.py:56",
    "default_value": "4",
    "value_range": "[2, 8]",
    "category": "Experiment Planning",
    "description": "Minimum spatial edge length (voxels) at the bottleneck feature map."
  },
  "target_spacing_percentile": {
    "location": "experiment_planner_baseline_3DUNet.py:58",
    "default_value": "50",
    "value_range": "[10, 90]",
    "category": "Experiment Planning",
    "description": "Percentile used to compute target voxel spacing from dataset statistics (50 = median)."
  },
  "anisotropy_threshold": {
    "location": "experiment_planner_baseline_3DUNet.py:59",
    "default_value": "3",
    "value_range": "[2, 10]",
    "category": "Experiment Planning",
    "description": "Spacing ratio threshold above which an axis is treated as anisotropic."
  },
  "batch_size_covers_max_percent": {
    "location": "experiment_planner_baseline_3DUNet.py:61",
    "default_value": "0.05",
    "value_range": "[0.01, 0.2]",
    "category": "Experiment Planning",
    "description": "Maximum fraction of the full dataset covered by a single batch."
  },
  "conv_per_stage_planner": {
    "location": "experiment_planner_baseline_3DUNet.py:64",
    "default_value": "2",
    "value_range": "[1, 4]",
    "category": "Experiment Planning",
    "description": "Convolutions per stage used in VRAM estimation for planning."
  },
  "default_num_threads": {
    "location": "configuration.py:3",
    "default_value": "8 or $nnUNet_def_n_proc",
    "value_range": "[1, 32]",
    "category": "Preprocessing",
    "description": "Worker threads for preprocessing and postprocessing. Controlled by env var nnUNet_def_n_proc."
  },
  "resampling_separate_z_aniso_threshold": {
    "location": "configuration.py:4",
    "default_value": "3",
    "value_range": "[2, 10]",
    "category": "Preprocessing",
    "description": "If max_spacing/min_spacing > threshold, the low-resolution z-axis is resampled independently with nearest-neighbor."
  },
  "resample_order_data": {
    "location": "preprocessing.py \u2192 GenericPreprocessor",
    "default_value": "3",
    "value_range": "[0, 5]",
    "category": "Preprocessing",
    "description": "Spline interpolation order for image data resampling (3 = cubic)."
  },
  "resample_order_seg": {
    "location": "preprocessing.py \u2192 GenericPreprocessor",
    "default_value": "1 (aniso axis) | 0 (iso axes)",
    "value_range": "[0, 3]",
    "category": "Preprocessing",
    "description": "Spline interpolation order for segmentation resampling (0 = nearest neighbor on iso axes)."
  },
  "ct_clip_percentile_low": {
    "location": "preprocessing.py (CT normalization branch)",
    "default_value": "0.5",
    "value_range": "[0.0, 5.0]",
    "category": "Preprocessing",
    "description": "Lower intensity percentile for CT clipping before normalization."
  },
  "ct_clip_percentile_high": {
    "location": "preprocessing.py (CT normalization branch)",
    "default_value": "99.5",
    "value_range": "[95.0, 100.0]",
    "category": "Preprocessing",
    "description": "Upper intensity percentile for CT clipping before normalization."
  },
  "batch_size_forced_fast": {
    "location": "nnUNetTrainerV2_fast.py \u2192 process_plans:31 | nnUNetTrainerV2_configurable.__init__:203 | nnUNetTrainerV2_configurable.py \u2192 process_plans:698",
    "default_value": "16",
    "value_range": "[2, 32]",
    "category": "Custom Trainer (fast)",
    "description": "Batch size forced to 16 by nnUNetTrainerV2_fast and nnUNetTrainerV2_configurable, overriding plans-derived batch size.",
    "active_value": 1
  }
}
