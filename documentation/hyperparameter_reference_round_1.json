{
  "max_num_epochs": {
    "location": "network_trainer.py → NetworkTrainer.__init__:97 | overridden in nnUNetTrainerV2.__init__:48",
    "default_value": "1000 (NetworkTrainer) | 1000 (nnUNetTrainerV2) | 1000 (fast) | 8000 (fast_8000)",
    "value_range": "[100, 10000]",
    "category": "Training Loop Control",
    "description": "Maximum number of training epochs. nnUNetTrainerV2 always trains to this limit (no early stopping)."
  },
  "num_batches_per_epoch": {
    "location": "network_trainer.py → NetworkTrainer.__init__:98 | overridden in nnUNetTrainerV2_fast:11",
    "default_value": "250 (NetworkTrainer) | 32 (fast / fast_8000)",
    "value_range": "[8, 1000]",
    "category": "Training Loop Control",
    "description": "Number of training iterations (minibatches) per epoch."
  },
  "num_val_batches_per_epoch": {
    "location": "network_trainer.py → NetworkTrainer.__init__:99 | overridden in nnUNetTrainerV2_fast:12 | nnUNetTrainerV2_fast_8000:12",
    "default_value": "50 (NetworkTrainer) | 1 (fast) | 4 (fast_8000)",
    "value_range": "[1, 200]",
    "category": "Training Loop Control",
    "description": "Number of online validation iterations per epoch for loss tracking."
  },
  "also_val_in_tr_mode": {
    "location": "network_trainer.py → NetworkTrainer.__init__:100",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Training Loop Control",
    "description": "Also compute validation loss with the network in training mode (adds train=True val losses)."
  },
  "patience": {
    "location": "network_trainer.py → NetworkTrainer.__init__:91",
    "default_value": "50",
    "value_range": "[10, 500] or null to disable",
    "category": "Early Stopping",
    "description": "Epochs to wait without training loss MA improvement before early stopping. Disabled in nnUNetTrainerV2 by epoch override."
  },
  "val_eval_criterion_alpha": {
    "location": "network_trainer.py → NetworkTrainer.__init__:92",
    "default_value": "0.9",
    "value_range": "(0.5, 0.99)",
    "category": "Early Stopping",
    "description": "EMA coefficient for validation evaluation criterion moving average. Higher = smoother but slower to react."
  },
  "train_loss_MA_alpha": {
    "location": "network_trainer.py → NetworkTrainer.__init__:95",
    "default_value": "0.93",
    "value_range": "(0.7, 0.99)",
    "category": "Early Stopping",
    "description": "EMA coefficient for training loss moving average used for patience-based stopping."
  },
  "train_loss_MA_eps": {
    "location": "network_trainer.py → NetworkTrainer.__init__:96",
    "default_value": "5e-4",
    "value_range": "[1e-5, 1e-2]",
    "category": "Early Stopping",
    "description": "Minimum improvement in train loss MA required to reset patience counter."
  },
  "lr_threshold": {
    "location": "network_trainer.py → NetworkTrainer.__init__:101",
    "default_value": "1e-6",
    "value_range": "[1e-8, 1e-4]",
    "category": "Early Stopping",
    "description": "LR floor: early stopping only triggers if current LR is at or below this value."
  },
  "optimizer_type_nnUNetTrainer": {
    "location": "nnUNetTrainer.py → initialize_optimizer_and_scheduler:267",
    "default_value": "Adam (amsgrad=True)",
    "value_range": ["Adam", "SGD", "AdamW", "Ranger"],
    "category": "Optimizer",
    "description": "Optimizer used in base nnUNetTrainer. Replaced by SGD in nnUNetTrainerV2."
  },
  "optimizer_type_nnUNetTrainerV2": {
    "location": "nnUNetTrainerV2.py → initialize_optimizer_and_scheduler:166",
    "default_value": "SGD",
    "value_range": ["SGD", "Adam", "AdamW"],
    "category": "Optimizer",
    "description": "Optimizer used in nnUNetTrainerV2. SGD with high momentum and poly-LR schedule."
  },
  "initial_lr_nnUNetTrainer": {
    "location": "nnUNetTrainer.py → __init__:126",
    "default_value": "3e-4",
    "value_range": "[1e-5, 1e-2]",
    "category": "Optimizer",
    "description": "Initial learning rate for Adam optimizer in base nnUNetTrainer."
  },
  "initial_lr_nnUNetTrainerV2": {
    "location": "nnUNetTrainerV2.py → __init__:49",
    "default_value": "1e-2",
    "value_range": "[1e-3, 3e-2]",
    "category": "Optimizer",
    "description": "Initial learning rate for SGD optimizer in nnUNetTrainerV2 with poly-LR decay."
  },
  "weight_decay": {
    "location": "nnUNetTrainer.py → __init__:127",
    "default_value": "3e-5",
    "value_range": "[1e-6, 1e-3]",
    "category": "Optimizer",
    "description": "L2 weight decay regularization, shared by both nnUNetTrainer and nnUNetTrainerV2."
  },
  "sgd_momentum": {
    "location": "nnUNetTrainerV2.py → initialize_optimizer_and_scheduler:167",
    "default_value": "0.99",
    "value_range": "[0.9, 0.999]",
    "category": "Optimizer",
    "description": "SGD momentum. Auto-reduced to 0.95 at epoch 100 if val Dice is 0."
  },
  "sgd_nesterov": {
    "location": "nnUNetTrainerV2.py → initialize_optimizer_and_scheduler:167",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Optimizer",
    "description": "Enable Nesterov momentum in SGD."
  },
  "adam_amsgrad": {
    "location": "nnUNetTrainer.py → initialize_optimizer_and_scheduler:268",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Optimizer",
    "description": "Use AMSGrad variant of Adam (base nnUNetTrainer only)."
  },
  "poly_lr_exponent": {
    "location": "nnUNetTrainerV2.py → maybe_update_lr:405 | poly_lr.py:17",
    "default_value": "0.9",
    "value_range": "(0.5, 1.0)",
    "category": "Learning Rate Schedule",
    "description": "Exponent for polynomial LR decay: lr = initial_lr * (1 - epoch/max_epochs)^exponent."
  },
  "lr_scheduler_type_nnUNetTrainer": {
    "location": "nnUNetTrainer.py → initialize_optimizer_and_scheduler:269",
    "default_value": "ReduceLROnPlateau",
    "value_range": ["ReduceLROnPlateau", "CosineAnnealingLR", "StepLR", "None"],
    "category": "Learning Rate Schedule",
    "description": "LR scheduler used in base nnUNetTrainer. Not used in nnUNetTrainerV2."
  },
  "lr_scheduler_factor": {
    "location": "nnUNetTrainer.py → initialize_optimizer_and_scheduler:270",
    "default_value": "0.2",
    "value_range": "(0.05, 0.5)",
    "category": "Learning Rate Schedule",
    "description": "Multiplicative factor for ReduceLROnPlateau LR reduction."
  },
  "lr_scheduler_patience": {
    "location": "nnUNetTrainer.py → __init__:125",
    "default_value": "30",
    "value_range": "[5, 100]",
    "category": "Learning Rate Schedule",
    "description": "Patience for ReduceLROnPlateau before reducing LR."
  },
  "lr_scheduler_eps": {
    "location": "nnUNetTrainer.py → __init__:124",
    "default_value": "1e-3",
    "value_range": "[1e-5, 1e-2]",
    "category": "Learning Rate Schedule",
    "description": "Threshold for ReduceLROnPlateau (threshold_mode=abs)."
  },
  "momentum_rescue_threshold": {
    "location": "nnUNetTrainerV2.py → on_epoch_end:418-420",
    "default_value": "0.95 (applied when val Dice = 0 at epoch 100)",
    "value_range": "(0.9, 0.99)",
    "category": "Learning Rate Schedule",
    "description": "If foreground Dice is still 0 at epoch 100, momentum is reduced from 0.99 to 0.95 and network weights are reinitialised."
  },
  "grad_clip_norm": {
    "location": "nnUNetTrainerV2.py → run_iteration:254,264",
    "default_value": "12",
    "value_range": "[1, 50]",
    "category": "Gradient Clipping",
    "description": "Maximum gradient norm for torch.nn.utils.clip_grad_norm_. Applied every iteration in nnUNetTrainerV2."
  },
  "fp16": {
    "location": "network_trainer.py → NetworkTrainer.__init__:59",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Mixed Precision / Reproducibility",
    "description": "Enable AMP training (torch.cuda.amp.autocast + GradScaler). Also controls mixed-precision inference."
  },
  "deterministic": {
    "location": "network_trainer.py → NetworkTrainer.__init__:62-68",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Mixed Precision / Reproducibility",
    "description": "If true: seeds NumPy/PyTorch/CUDA to 12345, enables cudnn.deterministic, disables cudnn.benchmark."
  },
  "random_seed": {
    "location": "network_trainer.py → NetworkTrainer.__init__:63-66",
    "default_value": "12345",
    "value_range": "any integer",
    "category": "Mixed Precision / Reproducibility",
    "description": "Hardcoded seed for NumPy, torch.manual_seed, and torch.cuda.manual_seed_all."
  },
  "pin_memory": {
    "location": "nnUNetTrainerV2.py → __init__:53",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Mixed Precision / Reproducibility",
    "description": "Pin DataLoader memory for faster host-to-GPU transfer."
  },
  "save_every": {
    "location": "network_trainer.py → NetworkTrainer.__init__:122",
    "default_value": "50",
    "value_range": "[10, 200]",
    "category": "Checkpointing",
    "description": "Save intermediate checkpoint every N epochs."
  },
  "save_latest_only": {
    "location": "network_trainer.py → NetworkTrainer.__init__:123",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Checkpointing",
    "description": "Only keep model_latest.model; do not save per-epoch checkpoint files."
  },
  "save_intermediate_checkpoints": {
    "location": "network_trainer.py → NetworkTrainer.__init__:125",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Checkpointing",
    "description": "Periodically save model_latest.model every save_every epochs."
  },
  "save_best_checkpoint": {
    "location": "network_trainer.py → NetworkTrainer.__init__:126",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Checkpointing",
    "description": "Save model_best.model when validation criterion MA improves."
  },
  "save_final_checkpoint": {
    "location": "network_trainer.py → NetworkTrainer.__init__:127",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Checkpointing",
    "description": "Save model_final_checkpoint.model at end of training."
  },
  "loss_function": {
    "location": "nnUNetTrainer.py → __init__:108",
    "default_value": "DC_and_CE_loss",
    "value_range": ["DC_and_CE_loss", "DC_and_topk_loss", "GDL_and_CE_loss", "DC_and_BCE_loss", "SoftDiceLoss", "RobustCrossEntropyLoss"],
    "category": "Loss Function",
    "description": "Combined SoftDice + CrossEntropy loss. Wrapped with MultipleOutputLoss2 in nnUNetTrainerV2 for deep supervision."
  },
  "batch_dice": {
    "location": "nnUNetTrainer.py → __init__:107",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Loss Function",
    "description": "Compute Dice loss pooling across the entire batch (pseudo-volume) rather than per sample."
  },
  "dice_smooth": {
    "location": "nnUNetTrainer.py → __init__:108",
    "default_value": "1e-5",
    "value_range": "[0, 1e-1]",
    "category": "Loss Function",
    "description": "Laplace smoothing constant in SoftDice numerator and denominator."
  },
  "dice_do_bg": {
    "location": "nnUNetTrainer.py → __init__:108",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Loss Function",
    "description": "Include background class (index 0) in Dice computation. False improves focus on foreground."
  },
  "weight_dice": {
    "location": "dice_loss.py → DC_and_CE_loss.__init__",
    "default_value": "1",
    "value_range": "[0, 10]",
    "category": "Loss Function",
    "description": "Scalar weight applied to the Dice component of the combined loss."
  },
  "weight_ce": {
    "location": "dice_loss.py → DC_and_CE_loss.__init__",
    "default_value": "1",
    "value_range": "[0, 10]",
    "category": "Loss Function",
    "description": "Scalar weight applied to the CrossEntropy component of the combined loss."
  },
  "square_dice": {
    "location": "dice_loss.py → DC_and_CE_loss.__init__",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Loss Function",
    "description": "Use SoftDiceLossSquared (Milletari et al.) instead of standard SoftDiceLoss."
  },
  "log_dice": {
    "location": "dice_loss.py → DC_and_CE_loss.__init__",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Loss Function",
    "description": "Apply -log transform to the Dice loss to emphasize hard examples."
  },
  "topk_k": {
    "location": "TopK_loss.py → TopKLoss.__init__:24",
    "default_value": "10",
    "value_range": "[1, 50] (percent)",
    "category": "Loss Function",
    "description": "Top-K percentage of hardest voxels to include in TopK CrossEntropy loss."
  },
  "deep_supervision_enabled": {
    "location": "nnUNetTrainerV2.py → initialize_network:158",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Deep Supervision",
    "description": "Enable multi-scale deep supervision outputs from the Generic_UNet decoder."
  },
  "ds_loss_weight_base": {
    "location": "nnUNetTrainerV2.py → initialize:81",
    "default_value": "1/2^i (exponential decay per level)",
    "value_range": "custom array of positive floats summing to 1",
    "category": "Deep Supervision",
    "description": "Per-scale loss weights: scale i gets weight 1/2^i; lowest level(s) masked to 0 then renormalized."
  },
  "ds_lowest_levels_masked": {
    "location": "nnUNetTrainerV2.py → initialize:84",
    "default_value": "1 (lowest resolution level masked)",
    "value_range": "[0, num_pool-1]",
    "category": "Deep Supervision",
    "description": "Number of lowest-resolution deep supervision outputs excluded from loss computation."
  },
  "base_num_features_v21": {
    "location": "experiment_planner_baseline_3DUNet_v21.py:36",
    "default_value": "32",
    "value_range": "[8, 64] (multiples of 8 recommended for AMP)",
    "category": "Network Architecture",
    "description": "Initial number of feature maps. 32 (vs 30) chosen for AMP divisibility-by-8 speedup."
  },
  "base_num_features_base": {
    "location": "generic_UNet.py → Generic_UNet:171 | experiment_planner_baseline_3DUNet.py:52",
    "default_value": "30",
    "value_range": "[8, 64]",
    "category": "Network Architecture",
    "description": "Base feature map count for the default planner (non-v21)."
  },
  "max_num_features_3D": {
    "location": "generic_UNet.py → Generic_UNet:173",
    "default_value": "320",
    "value_range": "[64, 512]",
    "category": "Network Architecture",
    "description": "Cap on maximum number of feature maps for 3D networks."
  },
  "max_num_features_2D": {
    "location": "generic_UNet.py → Generic_UNet:179",
    "default_value": "480",
    "value_range": "[64, 512]",
    "category": "Network Architecture",
    "description": "Cap on maximum number of feature maps for 2D networks."
  },
  "feat_map_mul_on_downscale": {
    "location": "generic_UNet.py → Generic_UNet.__init__:185",
    "default_value": "2",
    "value_range": "[1, 4]",
    "category": "Network Architecture",
    "description": "Feature map doubling factor applied at each encoder pooling stage."
  },
  "num_conv_per_stage": {
    "location": "generic_UNet.py → Generic_UNet.__init__:184 | nnUNetTrainer.py → process_plans:391-392",
    "default_value": "2",
    "value_range": "[1, 4]",
    "category": "Network Architecture",
    "description": "Number of convolutional blocks per encoder and decoder stage."
  },
  "norm_op": {
    "location": "nnUNetTrainerV2.py → initialize_network:143,148",
    "default_value": "InstanceNorm3d (3D) / InstanceNorm2d (2D)",
    "value_range": ["InstanceNorm3d", "BatchNorm3d", "GroupNorm"],
    "category": "Network Architecture",
    "description": "Normalization layer applied after each convolution."
  },
  "norm_op_eps": {
    "location": "nnUNetTrainerV2.py → initialize_network:150",
    "default_value": "1e-5",
    "value_range": "[1e-7, 1e-3]",
    "category": "Network Architecture",
    "description": "Epsilon for numerical stability in InstanceNorm."
  },
  "norm_op_affine": {
    "location": "nnUNetTrainerV2.py → initialize_network:150",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Network Architecture",
    "description": "Learn affine scale (gamma) and shift (beta) parameters in InstanceNorm."
  },
  "dropout_p": {
    "location": "nnUNetTrainerV2.py → initialize_network:151",
    "default_value": "0",
    "value_range": "[0.0, 0.5]",
    "category": "Network Architecture",
    "description": "Dropout probability. Set to 0 by default (disabled). Decoder dropout separately controlled by dropout_in_localization."
  },
  "dropout_in_localization": {
    "location": "nnUNetTrainerV2.py → initialize_network:158",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Network Architecture",
    "description": "Apply dropout in the decoder/localization path. When false, dropout_p is forced to 0.0 in decoder."
  },
  "nonlin": {
    "location": "nnUNetTrainerV2.py → initialize_network:152",
    "default_value": "nn.LeakyReLU",
    "value_range": ["nn.LeakyReLU", "nn.ReLU", "nn.ELU", "nn.GELU", "nn.Mish"],
    "category": "Network Architecture",
    "description": "Activation function applied after each normalization layer."
  },
  "nonlin_negative_slope": {
    "location": "nnUNetTrainerV2.py → initialize_network:153",
    "default_value": "1e-2",
    "value_range": "[0.0, 0.5]",
    "category": "Network Architecture",
    "description": "Negative slope coefficient for LeakyReLU."
  },
  "convolutional_pooling": {
    "location": "nnUNetTrainerV2.py → initialize_network:158",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Network Architecture",
    "description": "Use strided convolution instead of MaxPooling for downsampling."
  },
  "convolutional_upsampling": {
    "location": "nnUNetTrainerV2.py → initialize_network:159",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Network Architecture",
    "description": "Use transposed convolution instead of bilinear/trilinear interpolation for upsampling."
  },
  "seg_output_use_bias": {
    "location": "nnUNetTrainerV2.py → initialize_network:159",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Network Architecture",
    "description": "Add bias to the final segmentation output convolution layer."
  },
  "he_init_neg_slope": {
    "location": "nnUNetTrainerV2.py → initialize_network:158 | initialization.py → InitWeights_He",
    "default_value": "1e-2",
    "value_range": "[0.0, 0.2]",
    "category": "Network Architecture",
    "description": "Negative slope parameter for Kaiming (He) normal weight initialization, matched to LeakyReLU slope."
  },
  "do_elastic": {
    "location": "default_data_augmentation.py:43 | overridden in nnUNetTrainerV2.py:385",
    "default_value": "true (default) | false (nnUNetTrainerV2)",
    "value_range": [true, false],
    "category": "Data Augmentation",
    "description": "Enable elastic deformation augmentation. Disabled in nnUNetTrainerV2."
  },
  "elastic_deform_alpha": {
    "location": "default_data_augmentation.py:43",
    "default_value": "(0.0, 900.0)",
    "value_range": "[(0, 100), (0, 2000)]",
    "category": "Data Augmentation",
    "description": "Alpha range for elastic deformation magnitude."
  },
  "elastic_deform_sigma": {
    "location": "default_data_augmentation.py:44",
    "default_value": "(9.0, 13.0)",
    "value_range": "[(5, 10), (10, 20)]",
    "category": "Data Augmentation",
    "description": "Sigma range for elastic deformation smoothness."
  },
  "p_eldef": {
    "location": "default_data_augmentation.py:45",
    "default_value": "0.2",
    "value_range": "[0.0, 0.5]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of applying elastic deformation."
  },
  "do_scaling": {
    "location": "default_data_augmentation.py:47",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Data Augmentation",
    "description": "Enable random scaling augmentation."
  },
  "scale_range": {
    "location": "default_data_augmentation.py:48 | overridden in nnUNetTrainerV2.py:384",
    "default_value": "(0.85, 1.25) default | (0.7, 1.4) nnUNetTrainerV2",
    "value_range": "[(0.5, 1.0), (1.0, 2.0)] pair",
    "category": "Data Augmentation",
    "description": "Min/max scaling factor range. nnUNetTrainerV2 uses wider range (0.7, 1.4)."
  },
  "p_scale": {
    "location": "default_data_augmentation.py:52",
    "default_value": "0.2",
    "value_range": "[0.0, 0.5]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of applying random scaling."
  },
  "independent_scale_factor_for_each_axis": {
    "location": "default_data_augmentation.py:49",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Data Augmentation",
    "description": "Scale each spatial axis independently rather than uniformly."
  },
  "do_rotation": {
    "location": "default_data_augmentation.py:54",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Data Augmentation",
    "description": "Enable random rotation augmentation."
  },
  "rotation_x_default": {
    "location": "default_data_augmentation.py:55",
    "default_value": "(-0.2618, 0.2618) radians = ±15 degrees",
    "value_range": "[(-pi/2, pi/2)]",
    "category": "Data Augmentation",
    "description": "Rotation range around x-axis in radians (default 3D augmentation)."
  },
  "rotation_y_default": {
    "location": "default_data_augmentation.py:56",
    "default_value": "(-0.2618, 0.2618) radians = ±15 degrees",
    "value_range": "[(-pi/2, pi/2)]",
    "category": "Data Augmentation",
    "description": "Rotation range around y-axis in radians."
  },
  "rotation_z_default": {
    "location": "default_data_augmentation.py:57",
    "default_value": "(-0.2618, 0.2618) radians = ±15 degrees",
    "value_range": "[(-pi/2, pi/2)]",
    "category": "Data Augmentation",
    "description": "Rotation range around z-axis in radians."
  },
  "rotation_x_V2": {
    "location": "nnUNetTrainerV2.py → setup_DA_params:353",
    "default_value": "(-0.5236, 0.5236) radians = ±30 degrees",
    "value_range": "[(-pi/2, pi/2)]",
    "category": "Data Augmentation",
    "description": "Increased x-axis rotation range in nnUNetTrainerV2."
  },
  "rotation_p_per_axis": {
    "location": "default_data_augmentation.py:58",
    "default_value": "1",
    "value_range": "[0.0, 1.0]",
    "category": "Data Augmentation",
    "description": "Probability of applying rotation to each individual axis."
  },
  "p_rot": {
    "location": "default_data_augmentation.py:59",
    "default_value": "0.2",
    "value_range": "[0.0, 0.5]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of applying rotation."
  },
  "do_gamma": {
    "location": "default_data_augmentation.py:64",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Data Augmentation",
    "description": "Enable gamma augmentation (power-law intensity transform)."
  },
  "gamma_range": {
    "location": "default_data_augmentation.py:65",
    "default_value": "(0.7, 1.5)",
    "value_range": "[(0.5, 1.0), (1.0, 2.5)] pair",
    "category": "Data Augmentation",
    "description": "Range for gamma values in gamma augmentation."
  },
  "gamma_retain_stats": {
    "location": "default_data_augmentation.py:65",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Data Augmentation",
    "description": "Rescale gamma-transformed image to match original mean and std."
  },
  "p_gamma": {
    "location": "default_data_augmentation.py:67",
    "default_value": "0.3",
    "value_range": "[0.0, 0.5]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of forward gamma augmentation."
  },
  "gamma_inverted_p": {
    "location": "data_augmentation_moreDA.py:83 (hardcoded)",
    "default_value": "0.1",
    "value_range": "[0.0, 0.3]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of inverted gamma augmentation (applied before forward gamma in moreDA)."
  },
  "do_mirror": {
    "location": "default_data_augmentation.py:69 | overridden in nnUNetTrainerV2_fast.py:16",
    "default_value": "true (default) | false (fast trainers)",
    "value_range": [true, false],
    "category": "Data Augmentation",
    "description": "Enable random mirroring augmentation during training and TTA during inference."
  },
  "mirror_axes": {
    "location": "default_data_augmentation.py:70",
    "default_value": "(0, 1, 2) for 3D | (0, 1) for 2D",
    "value_range": "any subset of (0,) (1,) (2,) (0,1) (0,2) (1,2) (0,1,2)",
    "category": "Data Augmentation",
    "description": "Axes along which random mirroring may be applied."
  },
  "dummy_2D": {
    "location": "default_data_augmentation.py:72",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Data Augmentation",
    "description": "Treat 3D patches as 2D slices for augmentation; used for highly anisotropic data."
  },
  "border_mode_data": {
    "location": "default_data_augmentation.py:74",
    "default_value": "constant",
    "value_range": ["constant", "nearest", "reflect", "wrap"],
    "category": "Data Augmentation",
    "description": "Border handling mode for spatial transform (rotation/scaling/elastic)."
  },
  "gaussian_noise_p": {
    "location": "data_augmentation_moreDA.py:79 (hardcoded)",
    "default_value": "0.1",
    "value_range": "[0.0, 0.3]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of Gaussian noise injection (moreDA only)."
  },
  "gaussian_blur_sigma_range": {
    "location": "data_augmentation_moreDA.py:80 (hardcoded)",
    "default_value": "(0.5, 1.0)",
    "value_range": "[(0.1, 0.5), (0.5, 2.0)] pair",
    "category": "Data Augmentation",
    "description": "Sigma range for Gaussian blur (moreDA only)."
  },
  "gaussian_blur_p_per_sample": {
    "location": "data_augmentation_moreDA.py:80 (hardcoded)",
    "default_value": "0.2",
    "value_range": "[0.0, 0.5]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of Gaussian blur (moreDA only)."
  },
  "brightness_multiplicative_range": {
    "location": "data_augmentation_moreDA.py:81 (hardcoded)",
    "default_value": "(0.75, 1.25)",
    "value_range": "[(0.5, 1.0), (1.0, 2.0)] pair",
    "category": "Data Augmentation",
    "description": "Range for multiplicative brightness augmentation (moreDA only)."
  },
  "brightness_multiplicative_p": {
    "location": "data_augmentation_moreDA.py:81 (hardcoded)",
    "default_value": "0.15",
    "value_range": "[0.0, 0.4]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of multiplicative brightness augmentation (moreDA only)."
  },
  "contrast_augmentation_p": {
    "location": "data_augmentation_moreDA.py:84 (hardcoded)",
    "default_value": "0.15",
    "value_range": "[0.0, 0.4]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of contrast augmentation (moreDA only)."
  },
  "simulate_lowres_zoom_range": {
    "location": "data_augmentation_moreDA.py:85 (hardcoded)",
    "default_value": "(0.5, 1.0)",
    "value_range": "[(0.25, 0.75), (0.5, 1.0)] pair",
    "category": "Data Augmentation",
    "description": "Zoom range for simulated low-resolution augmentation (moreDA only)."
  },
  "simulate_lowres_p_per_sample": {
    "location": "data_augmentation_moreDA.py:85 (hardcoded)",
    "default_value": "0.25",
    "value_range": "[0.0, 0.5]",
    "category": "Data Augmentation",
    "description": "Per-sample probability of low-resolution simulation (moreDA only)."
  },
  "do_additive_brightness": {
    "location": "default_data_augmentation.py:76",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Data Augmentation",
    "description": "Enable additive brightness augmentation (off by default)."
  },
  "additive_brightness_mu": {
    "location": "default_data_augmentation.py:79",
    "default_value": "0.0",
    "value_range": "[-0.5, 0.5]",
    "category": "Data Augmentation",
    "description": "Mean of additive brightness distribution."
  },
  "additive_brightness_sigma": {
    "location": "default_data_augmentation.py:80",
    "default_value": "0.1",
    "value_range": "[0.01, 0.5]",
    "category": "Data Augmentation",
    "description": "Std of additive brightness distribution."
  },
  "num_augmentation_threads": {
    "location": "default_data_augmentation.py:82",
    "default_value": "12 or $nnUNet_n_proc_DA",
    "value_range": "[1, 32]",
    "category": "Data Loading",
    "description": "Number of worker threads for MultiThreadedAugmenter. Controlled by env var nnUNet_n_proc_DA."
  },
  "num_cached_per_thread": {
    "location": "default_data_augmentation.py:83 | overridden in nnUNetTrainerV2.py:389",
    "default_value": "1 (default) | 2 (nnUNetTrainerV2)",
    "value_range": "[1, 8]",
    "category": "Data Loading",
    "description": "Batches pre-fetched and cached per augmentation thread."
  },
  "oversample_foreground_percent": {
    "location": "nnUNetTrainer.py → __init__:129",
    "default_value": "0.33",
    "value_range": "[0.0, 1.0]",
    "category": "Data Loading",
    "description": "Fraction of each batch guaranteed to contain foreground voxels for class balance."
  },
  "unpack_data": {
    "location": "nnUNetTrainer.py → __init__:76",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Data Loading",
    "description": "Decompress NPZ files to NPY before training for faster I/O at the cost of disk space."
  },
  "use_nondetMultiThreadedAugmenter": {
    "location": "nnUNetTrainerV2.py → initialize:112",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Data Loading",
    "description": "Use NonDetMultiThreadedAugmenter (faster but non-deterministic) instead of standard MultiThreadedAugmenter."
  },
  "n_cv_splits": {
    "location": "nnUNetTrainerV2.py → do_split:296",
    "default_value": "5",
    "value_range": "[3, 10]",
    "category": "Cross-Validation",
    "description": "Number of cross-validation folds."
  },
  "kfold_random_state": {
    "location": "nnUNetTrainerV2.py → do_split:296 (hardcoded)",
    "default_value": "12345",
    "value_range": "any integer",
    "category": "Cross-Validation",
    "description": "Fixed seed for KFold split to ensure reproducibility across runs."
  },
  "out_of_range_fold_train_ratio": {
    "location": "nnUNetTrainerV2.py → do_split:323 (hardcoded)",
    "default_value": "0.8",
    "value_range": "(0.5, 0.95)",
    "category": "Cross-Validation",
    "description": "Train fraction for fallback random split when requested fold index exceeds available splits."
  },
  "val_do_mirroring": {
    "location": "nnUNetTrainerV2.py → validate:182",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Inference & Validation",
    "description": "Enable test-time augmentation (TTA) via mirroring during full validation. Forced to false in fast trainers."
  },
  "val_use_sliding_window": {
    "location": "nnUNetTrainer.py → validate:526",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Inference & Validation",
    "description": "Use sliding window inference for full-resolution validation (vs fully convolutional)."
  },
  "val_step_size": {
    "location": "nnUNetTrainer.py → validate:526",
    "default_value": "0.5",
    "value_range": "(0.1, 1.0)",
    "category": "Inference & Validation",
    "description": "Fraction of patch size used as step between sliding window positions (0.5 = 50% overlap)."
  },
  "val_use_gaussian": {
    "location": "nnUNetTrainer.py → validate:526",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Inference & Validation",
    "description": "Apply Gaussian importance weighting map in sliding window to reduce border artifacts."
  },
  "val_save_softmax": {
    "location": "nnUNetTrainer.py → validate:526",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Inference & Validation",
    "description": "Save softmax probability maps as NPZ files alongside hard segmentations."
  },
  "val_all_in_gpu": {
    "location": "nnUNetTrainer.py → validate:526",
    "default_value": "false",
    "value_range": [true, false],
    "category": "Inference & Validation",
    "description": "Keep aggregated softmax on GPU throughout sliding window (requires sufficient VRAM)."
  },
  "inference_pad_border_mode": {
    "location": "nnUNetTrainer.py → __init__:118",
    "default_value": "constant",
    "value_range": ["constant", "reflect", "edge"],
    "category": "Inference & Validation",
    "description": "Padding mode applied at image borders during inference."
  },
  "inference_pad_constant_value": {
    "location": "nnUNetTrainer.py → __init__:119",
    "default_value": "0",
    "value_range": "any float (typically 0 for CT or background value)",
    "category": "Inference & Validation",
    "description": "Fill value used when pad_border_mode is constant during inference."
  },
  "output_interpolation_order": {
    "location": "nnUNetTrainer.py → validate:549",
    "default_value": "1",
    "value_range": "[0, 3]",
    "category": "Inference & Validation",
    "description": "Spline interpolation order for resampling softmax predictions back to original spacing."
  },
  "output_interpolation_order_z": {
    "location": "nnUNetTrainer.py → validate:550",
    "default_value": "0",
    "value_range": "[0, 1]",
    "category": "Inference & Validation",
    "description": "Spline interpolation order along z-axis for anisotropic output resampling."
  },
  "run_postprocessing_on_folds": {
    "location": "nnUNetTrainer.py → validate:529",
    "default_value": "true",
    "value_range": [true, false],
    "category": "Inference & Validation",
    "description": "Run connected component postprocessing (keep largest component per class) after validation."
  },
  "unet_base_num_features_planner": {
    "location": "experiment_planner_baseline_3DUNet.py:52 | experiment_planner_baseline_3DUNet_v21.py:36",
    "default_value": "30 (base) | 32 (v21)",
    "value_range": "[8, 64] (multiples of 8 for AMP)",
    "category": "Experiment Planning",
    "description": "Base feature count used during memory estimation in experiment planner."
  },
  "unet_max_num_filters_planner": {
    "location": "experiment_planner_baseline_3DUNet.py:53",
    "default_value": "320",
    "value_range": "[64, 512]",
    "category": "Experiment Planning",
    "description": "Maximum feature map count ceiling applied during architecture planning."
  },
  "unet_min_batch_size": {
    "location": "experiment_planner_baseline_3DUNet.py:55",
    "default_value": "2",
    "value_range": "[1, 8]",
    "category": "Experiment Planning",
    "description": "Minimum batch size planned by the experiment planner."
  },
  "unet_featuremap_min_edge_length": {
    "location": "experiment_planner_baseline_3DUNet.py:56",
    "default_value": "4",
    "value_range": "[2, 8]",
    "category": "Experiment Planning",
    "description": "Minimum spatial edge length (voxels) at the bottleneck feature map."
  },
  "target_spacing_percentile": {
    "location": "experiment_planner_baseline_3DUNet.py:58",
    "default_value": "50",
    "value_range": "[10, 90]",
    "category": "Experiment Planning",
    "description": "Percentile used to compute target voxel spacing from dataset statistics (50 = median)."
  },
  "anisotropy_threshold": {
    "location": "experiment_planner_baseline_3DUNet.py:59",
    "default_value": "3",
    "value_range": "[2, 10]",
    "category": "Experiment Planning",
    "description": "Spacing ratio threshold above which an axis is treated as anisotropic."
  },
  "batch_size_covers_max_percent": {
    "location": "experiment_planner_baseline_3DUNet.py:61",
    "default_value": "0.05",
    "value_range": "[0.01, 0.2]",
    "category": "Experiment Planning",
    "description": "Maximum fraction of the full dataset covered by a single batch."
  },
  "conv_per_stage_planner": {
    "location": "experiment_planner_baseline_3DUNet.py:64",
    "default_value": "2",
    "value_range": "[1, 4]",
    "category": "Experiment Planning",
    "description": "Convolutions per stage used in VRAM estimation for planning."
  },
  "default_num_threads": {
    "location": "configuration.py:3",
    "default_value": "8 or $nnUNet_def_n_proc",
    "value_range": "[1, 32]",
    "category": "Preprocessing",
    "description": "Worker threads for preprocessing and postprocessing. Controlled by env var nnUNet_def_n_proc."
  },
  "resampling_separate_z_aniso_threshold": {
    "location": "configuration.py:4",
    "default_value": "3",
    "value_range": "[2, 10]",
    "category": "Preprocessing",
    "description": "If max_spacing/min_spacing > threshold, the low-resolution z-axis is resampled independently with nearest-neighbor."
  },
  "resample_order_data": {
    "location": "preprocessing.py → GenericPreprocessor",
    "default_value": "3",
    "value_range": "[0, 5]",
    "category": "Preprocessing",
    "description": "Spline interpolation order for image data resampling (3 = cubic)."
  },
  "resample_order_seg": {
    "location": "preprocessing.py → GenericPreprocessor",
    "default_value": "1 (aniso axis) | 0 (iso axes)",
    "value_range": "[0, 3]",
    "category": "Preprocessing",
    "description": "Spline interpolation order for segmentation resampling (0 = nearest neighbor on iso axes)."
  },
  "ct_clip_percentile_low": {
    "location": "preprocessing.py (CT normalization branch)",
    "default_value": "0.5",
    "value_range": "[0.0, 5.0]",
    "category": "Preprocessing",
    "description": "Lower intensity percentile for CT clipping before normalization."
  },
  "ct_clip_percentile_high": {
    "location": "preprocessing.py (CT normalization branch)",
    "default_value": "99.5",
    "value_range": "[95.0, 100.0]",
    "category": "Preprocessing",
    "description": "Upper intensity percentile for CT clipping before normalization."
  },
  "batch_size_forced_fast": {
    "location": "custom_trainers/nnUNetTrainerV2_fast.py → process_plans:31",
    "default_value": "16",
    "value_range": "[2, 32]",
    "category": "Custom Trainer (fast)",
    "description": "Batch size forced to 16 by nnUNetTrainerV2_fast and nnUNetTrainerV2_fast_8000, overriding plans."
  }
}
